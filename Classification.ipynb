{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":720475,"sourceType":"datasetVersion","datasetId":369929}],"dockerImageVersionId":30513,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:44.355374Z","iopub.execute_input":"2023-06-28T11:16:44.355785Z","iopub.status.idle":"2023-06-28T11:16:44.361350Z","shell.execute_reply.started":"2023-06-28T11:16:44.355757Z","shell.execute_reply":"2023-06-28T11:16:44.360207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/uci-online-news-popularity-data-set/OnlineNewsPopularity.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:44.363854Z","iopub.execute_input":"2023-06-28T11:16:44.364281Z","iopub.status.idle":"2023-06-28T11:16:44.777600Z","shell.execute_reply.started":"2023-06-28T11:16:44.364241Z","shell.execute_reply":"2023-06-28T11:16:44.776753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:44.779163Z","iopub.execute_input":"2023-06-28T11:16:44.779771Z","iopub.status.idle":"2023-06-28T11:16:44.785845Z","shell.execute_reply.started":"2023-06-28T11:16:44.779739Z","shell.execute_reply":"2023-06-28T11:16:44.784738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:44.787214Z","iopub.execute_input":"2023-06-28T11:16:44.787703Z","iopub.status.idle":"2023-06-28T11:16:44.821436Z","shell.execute_reply.started":"2023-06-28T11:16:44.787666Z","shell.execute_reply":"2023-06-28T11:16:44.820393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:44.825058Z","iopub.execute_input":"2023-06-28T11:16:44.825387Z","iopub.status.idle":"2023-06-28T11:16:44.852642Z","shell.execute_reply.started":"2023-06-28T11:16:44.825353Z","shell.execute_reply":"2023-06-28T11:16:44.851382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:44.853865Z","iopub.execute_input":"2023-06-28T11:16:44.854709Z","iopub.status.idle":"2023-06-28T11:16:45.087135Z","shell.execute_reply.started":"2023-06-28T11:16:44.854669Z","shell.execute_reply":"2023-06-28T11:16:45.086028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop_duplicates()\nprint(data.shape)\n'''The resultant shape implies that there aren't\nany duplicates'''","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:45.088915Z","iopub.execute_input":"2023-06-28T11:16:45.089315Z","iopub.status.idle":"2023-06-28T11:16:45.204174Z","shell.execute_reply.started":"2023-06-28T11:16:45.089277Z","shell.execute_reply":"2023-06-28T11:16:45.203084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = data.isna().sum()\nn[n > 0]\n#the output implies that there aren't any duplicates","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:45.205331Z","iopub.execute_input":"2023-06-28T11:16:45.205759Z","iopub.status.idle":"2023-06-28T11:16:45.225031Z","shell.execute_reply.started":"2023-06-28T11:16:45.205721Z","shell.execute_reply":"2023-06-28T11:16:45.223988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.hist(figsize = (20,20))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:45.226058Z","iopub.execute_input":"2023-06-28T11:16:45.227066Z","iopub.status.idle":"2023-06-28T11:16:54.269120Z","shell.execute_reply.started":"2023-06-28T11:16:45.227036Z","shell.execute_reply":"2023-06-28T11:16:54.267875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor = data.corr()\nplt.figure(figsize = (20,20))\ndata1 = cor.where(np.tril(np.ones(cor.shape).astype(bool)))\nsns.heatmap(data1, cmap = 'Reds')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:54.270735Z","iopub.execute_input":"2023-06-28T11:16:54.271931Z","iopub.status.idle":"2023-06-28T11:16:56.330665Z","shell.execute_reply.started":"2023-06-28T11:16:54.271896Z","shell.execute_reply":"2023-06-28T11:16:56.329640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Eliminating white space Character from the feature names\ndata.columns = data.columns.str.replace(\" \",\"\")","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:56.332478Z","iopub.execute_input":"2023-06-28T11:16:56.332909Z","iopub.status.idle":"2023-06-28T11:16:56.338679Z","shell.execute_reply.started":"2023-06-28T11:16:56.332869Z","shell.execute_reply":"2023-06-28T11:16:56.337611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''n_tokens_content - Number of words in the content\nHowever if its minimum value is 0 it means that there are \narticles that do not have any content.\nSuch records should be dropped as their related attributes\npose no meaning to our analysis\nfind number of rows that contain 0 for n_tokens_content'''\n\nnum_of_no_words = data[data['n_tokens_content'] == 0].index\nprint(\"The number of news articles/items without words\",num_of_no_words.size)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:56.340192Z","iopub.execute_input":"2023-06-28T11:16:56.340623Z","iopub.status.idle":"2023-06-28T11:16:56.355188Z","shell.execute_reply.started":"2023-06-28T11:16:56.340584Z","shell.execute_reply":"2023-06-28T11:16:56.354019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dropping rows of articles that have zero words in the conten\ndata = data[data['n_tokens_content'] != 0]","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:56.356553Z","iopub.execute_input":"2023-06-28T11:16:56.357526Z","iopub.status.idle":"2023-06-28T11:16:56.372509Z","shell.execute_reply.started":"2023-06-28T11:16:56.357483Z","shell.execute_reply":"2023-06-28T11:16:56.371555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Since URL is a non-numeric attribute and will not add value\nto our analysis so we drop it from the dataset.\nAlso timedelta is a non-predictive attribute and not a feature\nof the data set so we can drop it from the dataset.\nDrop highly correlated attributes \"n_non_stop_unique_tokens\",\n\"n_non_stop_words\",\"kw_avg_min\".'''\ndata = data.drop('url',axis=1)\ndata = data.drop('timedelta',axis=1)\ndata= data.drop([\"n_non_stop_unique_tokens\",\"n_non_stop_words\",\"kw_avg_min\"],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:56.378782Z","iopub.execute_input":"2023-06-28T11:16:56.379835Z","iopub.status.idle":"2023-06-28T11:16:56.401131Z","shell.execute_reply.started":"2023-06-28T11:16:56.379800Z","shell.execute_reply":"2023-06-28T11:16:56.399673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['shares'].describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:56.402916Z","iopub.execute_input":"2023-06-28T11:16:56.403353Z","iopub.status.idle":"2023-06-28T11:16:56.425324Z","shell.execute_reply.started":"2023-06-28T11:16:56.403321Z","shell.execute_reply":"2023-06-28T11:16:56.424146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = data['shares'].mean()\nb = data['shares'].median()\nprint(\"Mean shares: \",a)\nprint(\"Median shares: \",b)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:56.426915Z","iopub.execute_input":"2023-06-28T11:16:56.427246Z","iopub.status.idle":"2023-06-28T11:16:56.435039Z","shell.execute_reply.started":"2023-06-28T11:16:56.427216Z","shell.execute_reply":"2023-06-28T11:16:56.433791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a new target variable\ndata['popularity'] = data['shares'].apply(lambda x: 0 if x <1400 else 1)\ndata.hist(column = 'popularity');","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:56.436278Z","iopub.execute_input":"2023-06-28T11:16:56.436773Z","iopub.status.idle":"2023-06-28T11:16:56.766161Z","shell.execute_reply.started":"2023-06-28T11:16:56.436742Z","shell.execute_reply":"2023-06-28T11:16:56.765084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# n_tokens_content & shares\nplt.figure(figsize = (10,5))\nax = sns.scatterplot(y = 'shares', x = 'n_tokens_content', data = data)\n\n#n_tokens_title & shares\nplt.figure(figsize = (10,5))\nax = sns.scatterplot(y = 'shares', x = 'n_tokens_title', data = data ,palette = 'muted');","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:56.767839Z","iopub.execute_input":"2023-06-28T11:16:56.768522Z","iopub.status.idle":"2023-06-28T11:16:57.521030Z","shell.execute_reply.started":"2023-06-28T11:16:56.768481Z","shell.execute_reply":"2023-06-28T11:16:57.519901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"days_of_week = data.columns[26:33]\nprint(days_of_week)\n\nUnpopular  =data[data['shares'] < a]\nPopular = data[data['shares'] >= a]\nUnpopular_day = Unpopular[days_of_week].sum().values\nPopular_day = Popular[days_of_week].sum().values\n\nfig = plt.figure(figsize = (13,5))\nplt.title(\"The mean count of popular/unpopular news over different days of week\", fontsize = 16)\n\nplt.bar(np.arange(len(days_of_week)),Popular_day,width = 0.3, align = 'center', color = 'tab:orange', label = 'Popular')\nplt.bar(np.arange(len(days_of_week)) - 0.3,Unpopular_day,width = 0.3, align = 'center', color = 'g', label = 'Unpopular')\n\nplt.xticks(np.arange(len(days_of_week)),days_of_week)\nplt.ylabel('COUNT',fontsize = 15)\nplt.xlabel('Days of Week',fontsize = 17)\n\nplt.legend(loc = 'upper right')\nplt.tight_layout()\nplt.show()\n\n#*******************************************************************************************************\n\nUnpopular = data[data['shares'] < b]\nPopular = data[data['shares'] >= b]\nUnpopular_day = Unpopular[days_of_week].sum().values\nPopular_day = Popular[days_of_week].sum().values\n\nfig = plt.figure(figsize = (13,5))\nplt.title(\"The median Count of popular/unpopular news over different day of week\", fontsize = 16)\n\nplt.bar(np.arange(len(days_of_week)),Popular_day,width = 0.3, align = 'center', color = 'tab:orange', label = 'Popular')\nplt.bar(np.arange(len(days_of_week)) - 0.3,Unpopular_day,width = 0.3,align = 'center',color = 'g', label = 'Unpopular')\n\nplt.xticks(np.arange(len(days_of_week)),days_of_week)\nplt.ylabel('COUNT',fontsize = 15)\nplt.xlabel('Days of Week',fontsize = 17)\n\nplt.legend(loc = 'upper right')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:57.522357Z","iopub.execute_input":"2023-06-28T11:16:57.522678Z","iopub.status.idle":"2023-06-28T11:16:58.285136Z","shell.execute_reply.started":"2023-06-28T11:16:57.522632Z","shell.execute_reply":"2023-06-28T11:16:58.283977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data_channel = data.columns[9:15]\nprint(Data_channel)\n\nUnpopular = data[data['shares'] < a]\nPopular = data[data['shares'] >= a]\nUnpopular_day = Unpopular[Data_channel].sum().values\nPopular_day = Popular[Data_channel].sum().values\nfig = plt.figure(figsize = (13,5))\nplt.title(\"The mean count of popular/unpopular news over different data channel\", fontsize = 16)\nplt.bar(np.arange(len(Data_channel)), Popular_day, width = 0.3, align = \"center\", color = 'tab:orange', \\\n          label = \"popular\")\nplt.bar(np.arange(len(Data_channel)) - 0.3, Unpopular_day, width = 0.3, align = \"center\", color = 'g', \\\n          label = \"unpopular\")\nplt.xticks(np.arange(len(Data_channel)), Data_channel)\nplt.ylabel(\"Count\", fontsize = 12)\nplt.xlabel(\"Article category\", fontsize = 12)\n    \nplt.legend(loc = 'upper right')\nplt.tight_layout()\nplt.show()\n\n\n#********************************************************************************************************\n\nUnpopular = data[data['shares'] < b]\nPopular = data[data['shares'] >= b]\nUnpopular_day = Unpopular[Data_channel].sum().values\nPopular_day = Popular[Data_channel].sum().values\nfig = plt.figure(figsize = (13,5))\nplt.title(\"Count of popular/unpopular news over different data channel (Median)\", fontsize = 16)\nplt.bar(np.arange(len(Data_channel)), Popular_day, width = 0.3, align = \"center\", color = 'tab:orange', \\\n          label = \"popular\")\nplt.bar(np.arange(len(Data_channel)) - 0.3, Unpopular_day, width = 0.3, align = \"center\", color = 'g', \\\n          label = \"unpopular\")\nplt.xticks(np.arange(len(Data_channel)), Data_channel)\nplt.ylabel(\"Count\", fontsize = 12)\nplt.xlabel(\"Article category\", fontsize = 12)\n    \nplt.legend(loc = 'upper right')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:58.286461Z","iopub.execute_input":"2023-06-28T11:16:58.286808Z","iopub.status.idle":"2023-06-28T11:16:58.945775Z","shell.execute_reply.started":"2023-06-28T11:16:58.286778Z","shell.execute_reply":"2023-06-28T11:16:58.944717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in data:\n    plt.figure(figsize = (15,15))\n    sns.boxplot(data = data, x = column)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:16:58.947022Z","iopub.execute_input":"2023-06-28T11:16:58.947352Z","iopub.status.idle":"2023-06-28T11:17:13.466286Z","shell.execute_reply.started":"2023-06-28T11:16:58.947325Z","shell.execute_reply":"2023-06-28T11:17:13.465187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking for outliers and how many of them are present\nnum_cols = data.select_dtypes(['int64','float64']).columns\nfor column in num_cols:    \n    q1 = data[column].quantile(0.25)    # First Quartile\n    q3 = data[column].quantile(0.75)    # Third Quartile\n    IQR = q3 - q1                       # Inter Quartile Range\n\n    llimit = q1 - 1.5*IQR               # Lower Limit\n    ulimit = q3 + 1.5*IQR               # Upper Limit\n\n    outliers = data[(data[column] < llimit) | (data[column] > ulimit)]\n    print('Number of outliers in \"' + column + '\" : ' + str(len(outliers)))\n    print( llimit)\n    print( ulimit)\n    print( IQR)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:13.467698Z","iopub.execute_input":"2023-06-28T11:17:13.468082Z","iopub.status.idle":"2023-06-28T11:17:13.745082Z","shell.execute_reply.started":"2023-06-28T11:17:13.468051Z","shell.execute_reply":"2023-06-28T11:17:13.743831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"# df_num  dataframe contains numerical feaures.\n\ndf_num = data.drop([\"weekday_is_monday\",\"weekday_is_tuesday\",\"weekday_is_wednesday\",\"weekday_is_thursday\",\n                  \"weekday_is_friday\",\"weekday_is_saturday\",\"weekday_is_sunday\",\"is_weekend\",                  \n                  \"data_channel_is_lifestyle\",\"data_channel_is_entertainment\",\"data_channel_is_bus\",\n                  \"data_channel_is_socmed\",\"data_channel_is_tech\",\"data_channel_is_world\"],axis = 1)\n\n# df_cat dataframe contains catagorical features.\n\ndf_cat = data[[\"weekday_is_monday\",\"weekday_is_tuesday\",\"weekday_is_wednesday\",\"weekday_is_thursday\",\n             \"weekday_is_friday\",\"weekday_is_saturday\",\"weekday_is_sunday\",\"is_weekend\",            \n             \"data_channel_is_lifestyle\",\"data_channel_is_entertainment\",\"data_channel_is_bus\",\n                  \"data_channel_is_socmed\",\"data_channel_is_tech\",\"data_channel_is_world\"]]\n\n# Checking distribution of attributes to decide the method of scaling\n# Drop 'shares' from df_num\ndf_num = df_num.drop('shares', axis= 1)\n\ndf_num.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:13.746428Z","iopub.execute_input":"2023-06-28T11:17:13.746776Z","iopub.status.idle":"2023-06-28T11:17:13.766258Z","shell.execute_reply.started":"2023-06-28T11:17:13.746746Z","shell.execute_reply":"2023-06-28T11:17:13.764979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Searching for negative values in columns, if any.\nnegative_colns = df_num.columns[(df_num <= 0).any()]\nprint(negative_colns)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:13.767915Z","iopub.execute_input":"2023-06-28T11:17:13.769062Z","iopub.status.idle":"2023-06-28T11:17:13.782214Z","shell.execute_reply.started":"2023-06-28T11:17:13.769021Z","shell.execute_reply":"2023-06-28T11:17:13.781010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting the negative values into positive so that\n#Box - Cox method can be applied\nfor i in negative_colns:\n    m=df_num[i].min()\n    name=i +'_new'\n    df_num[name]=((df_num[i]+1)-m)\n    \ndf_num.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:13.784236Z","iopub.execute_input":"2023-06-28T11:17:13.784546Z","iopub.status.idle":"2023-06-28T11:17:13.834003Z","shell.execute_reply.started":"2023-06-28T11:17:13.784520Z","shell.execute_reply":"2023-06-28T11:17:13.832951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Droping the previous negative columns\nfor i in negative_colns:\n    df_num.drop(i,axis=1,inplace=True)\n\nnegative_colns=df_num.columns[(df_num<=0).any()]\nprint(negative_colns)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:13.835405Z","iopub.execute_input":"2023-06-28T11:17:13.835831Z","iopub.status.idle":"2023-06-28T11:17:14.134534Z","shell.execute_reply.started":"2023-06-28T11:17:13.835802Z","shell.execute_reply":"2023-06-28T11:17:14.133461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#HANDLING THE OUTLIERS\n      \npt = preprocessing.PowerTransformer(method = 'box-cox',standardize = False)\ndf_num_add = pt.fit_transform(df_num)\ndf_num_add = (pd.DataFrame(df_num_add,columns = df_num.columns))\n\nfor col in df_num_add.columns:\n    percentiles = df_num_add[col].quantile([0.01,0.99]).values\n    df_num_add[col][df_num_add[col] <= percentiles[0]] = percentiles[0]\n    df_num_add[col][df_num_add[col] >= percentiles[1]] = percentiles[1]\n\n\n# Checking for the outliers again\nnum_cols = df_num_add.select_dtypes(['int64','float64']).columns\n\nfor column in num_cols:    \n    q1 = df_num_add[column].quantile(0.25)   # First Quartile (Q1)\n    q3 = df_num_add[column].quantile(0.75)   # Third Quartile (Q3)\n    IQR = q3 - q1                            # Inter Quartile Range (IQR)\n\n    llimit = q1 - 1.5*IQR                    # Lower Limit\n    ulimit = q3 + 1.5*IQR                    # Upper Limit\n\n    outliers = df_num_add[(df_num_add[column] < llimit) | (df_num_add[column] > ulimit)]\n    print('Number of outliers in \"' + column + '\" : ' + str(len(outliers)))\n    print( llimit) #Lower limit\n    print(ulimit) #Upper limit\n    print( IQR) #Inter Quartile Range","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:14.136069Z","iopub.execute_input":"2023-06-28T11:17:14.136402Z","iopub.status.idle":"2023-06-28T11:17:15.669762Z","shell.execute_reply.started":"2023-06-28T11:17:14.136373Z","shell.execute_reply":"2023-06-28T11:17:15.668939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nnum_cols = df_num_add.select_dtypes(['int64', 'float64']).columns\n\nfor col in num_cols:\n    sns.boxplot(x=col, data=df_num_add)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:15.671116Z","iopub.execute_input":"2023-06-28T11:17:15.671544Z","iopub.status.idle":"2023-06-28T11:17:22.566704Z","shell.execute_reply.started":"2023-06-28T11:17:15.671504Z","shell.execute_reply":"2023-06-28T11:17:22.565585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_num_add.columns)\nprint(df_cat.columns)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.568601Z","iopub.execute_input":"2023-06-28T11:17:22.569350Z","iopub.status.idle":"2023-06-28T11:17:22.579052Z","shell.execute_reply.started":"2023-06-28T11:17:22.569313Z","shell.execute_reply":"2023-06-28T11:17:22.578045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final = pd.concat([df_num_add,df_cat], axis = 1)\ndf_final.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.581718Z","iopub.execute_input":"2023-06-28T11:17:22.582784Z","iopub.status.idle":"2023-06-28T11:17:22.601816Z","shell.execute_reply.started":"2023-06-28T11:17:22.582743Z","shell.execute_reply":"2023-06-28T11:17:22.600786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final['popularity'] = data['shares'].apply(lambda x: 0 if x <1400 else 1)\ndf_final.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.604380Z","iopub.execute_input":"2023-06-28T11:17:22.605026Z","iopub.status.idle":"2023-06-28T11:17:22.636303Z","shell.execute_reply.started":"2023-06-28T11:17:22.604992Z","shell.execute_reply":"2023-06-28T11:17:22.635313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final=df_final.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.637427Z","iopub.execute_input":"2023-06-28T11:17:22.637753Z","iopub.status.idle":"2023-06-28T11:17:22.656674Z","shell.execute_reply.started":"2023-06-28T11:17:22.637726Z","shell.execute_reply":"2023-06-28T11:17:22.655706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.columns","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.657702Z","iopub.execute_input":"2023-06-28T11:17:22.658006Z","iopub.status.idle":"2023-06-28T11:17:22.665823Z","shell.execute_reply.started":"2023-06-28T11:17:22.657979Z","shell.execute_reply":"2023-06-28T11:17:22.664668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.666984Z","iopub.execute_input":"2023-06-28T11:17:22.667275Z","iopub.status.idle":"2023-06-28T11:17:22.677542Z","shell.execute_reply.started":"2023-06-28T11:17:22.667250Z","shell.execute_reply":"2023-06-28T11:17:22.676392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification Models to be used\n**1. AdaBoost CLassifier**    \n**2. Logisitic Regression**    \n**3. Random Forest**    \n**4. GaussianNB**    \n**5. SVC**  \n**6. KNeighborsClassifier**    \n**7. Decision Tree**","metadata":{}},{"cell_type":"code","source":"modelscore=[]\nX=df_final.drop(['popularity','popularity_new'],axis=1)\ny=df_final['popularity']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.679170Z","iopub.execute_input":"2023-06-28T11:17:22.679456Z","iopub.status.idle":"2023-06-28T11:17:22.711762Z","shell.execute_reply.started":"2023-06-28T11:17:22.679431Z","shell.execute_reply":"2023-06-28T11:17:22.710580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n\n# learner: the learning algorithm to be trained and predicted on\n# sample_size: the size of samples (number) to be drawn from training set\n# X_train: features training set\n# y_train: income training set\n# X_test: features testing set\n# y_test: income testing set\n    \n    results = {}\n    \n    start = time() # start time\n    learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time() # end time\n\n    results['train_time'] = end-start\n        \n    # Get predictions on the first 4000 training samples\n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:4000])\n    end = time() # Get end time\n    \n    # he total prediction time\n    results['pred_time'] = end-start\n    # Compute accuracy on the first 4000 training samples\n    results['acc_train'] = accuracy_score(y_train[:4000],predictions_train)     \n    # Compute accuracy on test set\n    results['acc_test'] = accuracy_score(y_test,predictions_test)\n    # Compute F-score on the the first 4000 training samples\n    results['f_train'] = fbeta_score(y_train[:4000],predictions_train,beta=1)   \n    # Compute F-score on the test set\n    results['f_test'] = fbeta_score(y_test,predictions_test,beta=1)\n    # Compute AUC on the the first 4000 training samples\n    results['auc_train'] = roc_auc_score(y_train[:4000],predictions_train)  \n    # Compute AUC on the test set\n    results['auc_test'] = roc_auc_score(y_test,predictions_test)\n       \n    # Success\n    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n    print (\"{} with accuracy {}, F1 {} and AUC {}.\".format(learner.__class__.__name__,\\\n          results['acc_test'],results['f_test'], results['auc_test']) )\n\n\n    #plt.show()\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.713038Z","iopub.execute_input":"2023-06-28T11:17:22.713424Z","iopub.status.idle":"2023-06-28T11:17:22.723384Z","shell.execute_reply.started":"2023-06-28T11:17:22.713396Z","shell.execute_reply":"2023-06-28T11:17:22.722157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.patches as mpatches\ndef evaluate(results,name):\n    \n# learners: a list of supervised learners\n# stats: a list of dictionaries of the statistic results from 'train_predict()'\n# accuracy: The score for the naive predictor\n# f1: The score for the naive predictor\n  \n    # Create figure\n    fig, ax = plt.subplots(2, 4, figsize = (16,7))\n\n    bar_width = 0.3\n    colors = ['#A00000','#00A0A0','#00A000']\n    \n    \n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'auc_train','pred_time', 'acc_test',\\\n                                    'f_test', 'auc_test']):\n            for i in np.arange(3):\n                \n                # plot code\n                ax[j//4, j%4].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n                ax[j//4, j%4].set_xticks([0.45, 1.45, 2.45])\n                ax[j//4, j%4].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n                ax[j//4, j%4].set_xlim((-0.1, 3.0))\n    \n    # Add labels\n    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n    ax[0, 1].set_ylabel(\"Accuracy Score\")\n    ax[0, 2].set_ylabel(\"F-score\")\n    ax[0, 3].set_ylabel(\"AUC\")\n    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n    ax[1, 1].set_ylabel(\"Accuracy Score\")\n    ax[1, 2].set_ylabel(\"F-score\")\n    ax[1, 3].set_ylabel(\"AUC\")\n    ax[1, 0].set_xlabel(\"Training Set Size\")\n    ax[1, 1].set_xlabel(\"Training Set Size\")\n    ax[1, 2].set_xlabel(\"Training Set Size\")\n    ax[1, 3].set_xlabel(\"Training Set Size\")\n    \n    # Add titles\n    ax[0, 0].set_title(\"Model Training\")\n    ax[0, 1].set_title(\"Accuracy Score on Training Subset\")\n    ax[0, 2].set_title(\"F-score on Training Subset\")\n    ax[0, 3].set_title(\"AUC on Training Subset\")\n    ax[1, 0].set_title(\"Model Predicting\")\n    ax[1, 1].set_title(\"Accuracy Score on Testing Set\")\n    ax[1, 2].set_title(\"F-score on Testing Set\")\n    ax[1, 3].set_title(\"AUC on Testing Subset\")\n    \n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[0, 2].set_ylim((0, 1))\n    ax[0, 3].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n    ax[1, 2].set_ylim((0, 1))\n    ax[1, 3].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    plt.legend(handles = patches,  bbox_to_anchor = (-1.4, 2.54),\\\n               loc = 'upper center', borderaxespad = 0., ncol = 3, fontsize = 'x-large')\n    \n    plt.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.10)\n    plt.savefig(name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.724634Z","iopub.execute_input":"2023-06-28T11:17:22.724938Z","iopub.status.idle":"2023-06-28T11:17:22.742365Z","shell.execute_reply.started":"2023-06-28T11:17:22.724913Z","shell.execute_reply":"2023-06-28T11:17:22.741275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost CLassifier, Logisitic Regression, Random Forest","metadata":{}},{"cell_type":"code","source":"# Import the three supervised learning models from sklearn\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom time import time\nfrom IPython.display import display\nfrom sklearn.metrics import accuracy_score, fbeta_score, roc_curve, auc, roc_auc_score\n# Initialize the three models\nclf_A = AdaBoostClassifier(random_state=0)\nclf_B = LogisticRegression(random_state=0,C=1.0)\nclf_C = RandomForestClassifier(random_state=0)\n\n# Calculate the number of samples for 1%, 10%, and 100% of the training data\nsamples_1 = int(X_train.shape[0]*0.01)\nsamples_10 = int(X_train.shape[0]*0.1)\nsamples_100 = X_train.shape[0]\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        if clf == clf_A:\n            results[clf_name][i] = \\\n            train_predict(clf, samples, X_train, y_train, X_test, y_test)\n        elif clf == clf_B:\n            results[clf_name][i] = \\\n            train_predict(clf, samples, X_train, y_train, X_test, y_test)\n        else:\n            results[clf_name][i] = \\\n            train_predict(clf, samples, X_train, y_train, X_test, y_test)\n\n# Run metrics visualization for the three supervised learning models chosen\nevaluate(results,'perf_unopt.pdf')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:22.743453Z","iopub.execute_input":"2023-06-28T11:17:22.744132Z","iopub.status.idle":"2023-06-28T11:17:53.598517Z","shell.execute_reply.started":"2023-06-28T11:17:22.744103Z","shell.execute_reply":"2023-06-28T11:17:53.597372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GaussianNB, Support Vector Classifier(SVC), KNeighborsClassifier","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_A =GaussianNB()\nclf_B = SVC(random_state=0,C=1.0)\nclf_C = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)\n\n# Calculate the number of samples for 1%, 10%, and 100% of the training data\nsamples_1 = int(X_train.shape[0]*0.01)\nsamples_10 = int(X_train.shape[0]*0.1)\nsamples_100 = X_train.shape[0]\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        if clf == clf_A:\n            results[clf_name][i] = \\\n            train_predict(clf, samples, X_train, y_train, X_test, y_test)\n        elif clf == clf_B:\n            results[clf_name][i] = \\\n            train_predict(clf, samples, X_train, y_train, X_test, y_test)\n        else:\n            results[clf_name][i] = \\\n            train_predict(clf, samples, X_train, y_train, X_test, y_test)\n\n# Run metrics visualization for the three supervised learning models chosen\nevaluate(results,'perf_unopt1.pdf')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:17:53.600217Z","iopub.execute_input":"2023-06-28T11:17:53.600877Z","iopub.status.idle":"2023-06-28T11:19:39.351549Z","shell.execute_reply.started":"2023-06-28T11:17:53.600836Z","shell.execute_reply":"2023-06-28T11:19:39.350340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(random_state=0)\n\n\n# Calculate the number of samples for 1%, 10%, and 100% of the training data\nsamples_1 = int(X_train.shape[0] * 0.01)\nsamples_10 = int(X_train.shape[0] * 0.1)\nsamples_100 = X_train.shape[0]\n\n# Collect results on the learner\nresults = {}\nclf_name = clf.__class__.__name__\nresults[clf_name] = {}\n\nfor i, samples in enumerate([samples_1, samples_10, samples_100]):\n    results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_test, y_test)\n    \n\n# Run metrics visualization for the supervised learning model\nevaluate(results, 'perf_unopt1.pdf')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:19:39.353141Z","iopub.execute_input":"2023-06-28T11:19:39.353539Z","iopub.status.idle":"2023-06-28T11:19:43.306209Z","shell.execute_reply.started":"2023-06-28T11:19:39.353504Z","shell.execute_reply":"2023-06-28T11:19:43.305048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature selection \n# USING BACKWARD ELIMINATION\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\ncols = df_final.columns\nmodel = RandomForestClassifier()\nrfe = RFE(model, n_features_to_select=57)\n\n# Transforming data using RFE\nX_rfe = rfe.fit_transform(X, y)\n\n# Fitting the data to the model\nmodel.fit(X_rfe, y)\n\nprint(\"Selected Features:\")\nselected_features = [col for col, support in zip(cols, rfe.support_) if support]\nprint(selected_features)\n#print(rfe.support_)\nprint(\"Feature Rankings:\")\nprint(rfe.ranking_)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:19:43.313796Z","iopub.execute_input":"2023-06-28T11:19:43.314435Z","iopub.status.idle":"2023-06-28T11:20:30.405899Z","shell.execute_reply.started":"2023-06-28T11:19:43.314390Z","shell.execute_reply":"2023-06-28T11:20:30.404606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best model till now :Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import  train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n\nrf=RandomForestClassifier()\nrf.fit(x_train,y_train)\ny_pred_train=rf.predict(x_train)\ny_prob_train=rf.predict_proba(x_train)[:,1]\n\ny_pred=rf.predict(x_test)\ny_prob=rf.predict_proba(x_test)[:,1]  #used to find AUC of train and test\n\nfrom sklearn.metrics import accuracy_score,roc_curve,roc_auc_score, classification_report\n\n#print('Accuracy of Random forest train :',accuracy_score(y_pred_train,y_train))\n#print('Accuracy of Random forest test:',accuracy_score(y_pred,y_test))\n#print('AUC of Random forest train :',roc_auc_score(y_train,y_prob_train))\n#print('AUC of Random forest test :',roc_auc_score(y_test,y_prob))\n\n# Generate and print the classification report\nclassification_report_train = classification_report(y_train, y_pred_train)\nclassification_report_test = classification_report(y_test, y_pred)\nprint('Classification Report - Training Set:')\nprint(classification_report_train)\nprint('Classification Report - Testing Set:')\nprint(classification_report_test)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:20:30.407065Z","iopub.execute_input":"2023-06-28T11:20:30.407376Z","iopub.status.idle":"2023-06-28T11:20:49.138242Z","shell.execute_reply.started":"2023-06-28T11:20:30.407343Z","shell.execute_reply":"2023-06-28T11:20:49.137195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.xticks([0.5, 1.5], [\"Unpopular\", \"Unpopular\"])\n    plt.yticks([0.5, 1.5], [\"Unpopular\", \"Unpopular\"])\n    plt.show()\nplot_confusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-06-28T11:20:49.139797Z","iopub.execute_input":"2023-06-28T11:20:49.140218Z","iopub.status.idle":"2023-06-28T11:20:49.394402Z","shell.execute_reply.started":"2023-06-28T11:20:49.140177Z","shell.execute_reply":"2023-06-28T11:20:49.392096Z"},"trusted":true},"execution_count":null,"outputs":[]}]}